# PDF to Markdown MCP Server

## Project Overview

GPU-accelerated PDF processing system that converts PDFs to markdown with vector embeddings. Uses MinerU standalone service for CUDA-accelerated processing, Celery for task orchestration, and PostgreSQL with PGVector for embeddings storage.

**Key Capabilities:**
- Watches directories for new PDFs (configured via API)
- GPU-accelerated conversion (5-10x faster than CPU)
- Vector embeddings for semantic search
- **MCP semantic search tool** - `search_library` for LLMs
- MCP-compatible REST API
- Persistent configuration survives restarts

## Tech Stack

- **Language**: Python 3.11+
- **Framework**: FastAPI + Celery + FastMCP
- **Database**: PostgreSQL 17+ with PGVector
- **Queue**: Redis (Docker)
- **GPU**: CUDA 12.4+ (MinerU)
- **Embeddings**: Ollama (nomic-embed-text)
- **Process Management**: systemd services
- **MCP Server**: FastMCP with semantic search tool

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MCP Client / API Consumer       ‚îÇ
‚îÇ Configure via HTTP API          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ HTTP/REST :8000
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FastAPI Server                  ‚îÇ
‚îÇ - 4 uvicorn workers             ‚îÇ
‚îÇ - /api/v1/configure             ‚îÇ
‚îÇ - /api/v1/configuration         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚îú‚îÄ‚îÄ‚ñ∂ PostgreSQL (192.168.1.104)
             ‚îÇ   ‚îî‚îÄ server_configuration table
             ‚îÇ   ‚îî‚îÄ documents, document_content
             ‚îÇ   ‚îî‚îÄ document_embeddings (PGVector)
             ‚îÇ
             ‚îú‚îÄ‚îÄ‚ñ∂ Redis (Docker :6379)
             ‚îÇ   ‚îî‚îÄ mineru_requests queue
             ‚îÇ   ‚îî‚îÄ mineru_results queue
             ‚îÇ   ‚îî‚îÄ celery queue
             ‚îÇ   ‚îî‚îÄ embeddings queue
             ‚îÇ
             ‚îú‚îÄ‚îÄ‚ñ∂ MinerU Standalone Service
             ‚îÇ   ‚îî‚îÄ GPU-accelerated processing
             ‚îÇ   ‚îî‚îÄ Redis queue consumer
             ‚îÇ
             ‚îî‚îÄ‚îÄ‚ñ∂ Celery Workers
                 ‚îî‚îÄ pdf-celery-worker.service
                 ‚îî‚îÄ pdf-celery-beat.service
```

## Pipeline Mental Model

> **‚ö†Ô∏è IMPORTANT**: Keep this section updated as the system evolves. This is the source of truth for understanding data flow, bottlenecks, and performance characteristics.

### Complete Data Flow (PDF ‚Üí Embeddings)

```
1. FILE DISCOVERY (< 1 sec)
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Watchdog (worker/indexer.py)                    ‚îÇ
   ‚îÇ - Monitors watch directories                    ‚îÇ
   ‚îÇ - Triggers on new .pdf files                    ‚îÇ
   ‚îÇ - Creates document record in database           ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ conversion_status: 'pending'
                     ‚ñº

2. CELERY TASK QUEUE (< 1 sec)
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Redis Queue: "celery"                           ‚îÇ
   ‚îÇ - Task: process_pdf_document                    ‚îÇ
   ‚îÇ - Priority: Normal                              ‚îÇ
   ‚îÇ - Worker: pdf-celery-worker (solo pool)         ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ Task dequeued by worker
                     ‚ñº

3. PDF PROCESSING TASK (10-60 sec per PDF)
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Celery Worker (worker/tasks.py:process_pdf)     ‚îÇ
   ‚îÇ - Updates status: 'processing'                  ‚îÇ
   ‚îÇ - Checks GPU memory availability                ‚îÇ
   ‚îÇ - Calls MinerU client                           ‚îÇ
   ‚îÇ - ‚ö†Ô∏è BLOCKS HERE waiting for MinerU result      ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ Redis RPC call (synchronous)
                     ‚ñº

4. GPU PROCESSING (5-30 sec per PDF)
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ MinerU Standalone (services/mineru_standalone)  ‚îÇ
   ‚îÇ - Reads from Redis: mineru_requests_N           ‚îÇ
   ‚îÇ - Loads PDF into GPU memory                     ‚îÇ
   ‚îÇ - Stage 1: Layout detection (GPU)               ‚îÇ
   ‚îÇ - Stage 2: OCR processing (GPU)                 ‚îÇ
   ‚îÇ - Stage 3: Table/formula extraction (GPU)       ‚îÇ
   ‚îÇ - Stage 4: Markdown generation (CPU)            ‚îÇ
   ‚îÇ - Writes result to Redis: mineru_results_N      ‚îÇ
   ‚îÇ ‚ö†Ô∏è NO TIMING INSTRUMENTATION (BLIND SPOT)       ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ Result returned via Redis
                     ‚ñº

5. MARKDOWN WRITE (< 1 sec)
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Back to Celery Worker                           ‚îÇ
   ‚îÇ - Writes markdown to OUTPUT_DIRECTORY           ‚îÇ
   ‚îÇ - Option: Async file I/O (if enabled)           ‚îÇ
   ‚îÇ - Option: Batch database write (if enabled)     ‚îÇ
   ‚îÇ - Updates status: 'completed'                   ‚îÇ
   ‚îÇ - Queues downstream: generate_embeddings        ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ Async task queued
                     ‚ñº

6. EMBEDDING GENERATION (5-30 sec per document)
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Celery Worker (worker/tasks.py:generate_embeds) ‚îÇ
   ‚îÇ - Reads markdown content                        ‚îÇ
   ‚îÇ - Chunks text (1000 chars, 200 overlap)         ‚îÇ
   ‚îÇ - Batches chunks (batch_size=32)                ‚îÇ
   ‚îÇ - Calls Ollama API (concurrent requests=8)      ‚îÇ
   ‚îÇ - Writes embeddings to database                 ‚îÇ
   ‚îÇ ‚ö†Ô∏è INDIVIDUAL INSERTS (10-100x slow)            ‚îÇ
   ‚îÇ - Updates embedding_status: 'completed'         ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ DONE
                     ‚ñº
   ‚úÖ Document fully processed and searchable
```

### Performance Characteristics

| Stage | Expected Time | Bottleneck Type | Critical Path |
|-------|--------------|-----------------|---------------|
| File Discovery | < 1 sec | I/O | No |
| Queue Wait | < 5 sec | Concurrency | Yes (if backlog) |
| GPU Memory Check | 50-100 ms | Subprocess | Yes |
| MinerU Processing | 5-30 sec | GPU/CPU | **YES (CRITICAL)** |
| Markdown Write | < 1 sec | I/O | Yes (if sync) |
| Embedding Generation | 5-30 sec | Network/CPU | **YES (CRITICAL)** |
| Database Writes | < 1 sec | Database | Yes (if individual) |
| **Total (10-page PDF)** | **15-60 sec** | - | - |

### Component Interactions & Timing

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  1. Queue task      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Celery     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   Indexer    ‚îÇ
‚îÇ   Worker     ‚îÇ                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     < 1 sec
       ‚îÇ
       ‚îÇ 2. Request PDF processing (BLOCKING)
       ‚îÇ    worker/tasks.py:413-418
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  3. Push request    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   MinerU     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ    Redis     ‚îÇ
‚îÇ   Client     ‚îÇ                     ‚îÇ  Queue (RPC) ‚îÇ
‚îÇ              ‚îÇ  4. Block wait      ‚îÇ              ‚îÇ
‚îÇ              ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  (up to 300 sec)    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚ñ≤                                     ‚îÇ
       ‚îÇ                                     ‚îÇ 3. Pop request
       ‚îÇ 6. Return result                    ‚ñº
       ‚îÇ                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ                              ‚îÇ   MinerU     ‚îÇ
       ‚îÇ                              ‚îÇ  Standalone  ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  (3 GPU      ‚îÇ
         5. Push result to Redis      ‚îÇ   instances) ‚îÇ
                                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      5-30 sec (GPU)
```

### Known Bottlenecks & Optimization Targets

> **From Performance Review (2025-10)** - See performance review commits for details

#### üî¥ **Critical Bottlenecks**

1. **Synchronous Redis Blocking** (`services/mineru_client.py:122`)
   - **Impact**: Worker blocked for up to 300s waiting for result
   - **Effect**: Serializes all processing, prevents parallelism
   - **Solution**: Async Redis with callbacks or pub/sub
   - **Expected Gain**: 2-3x throughput

2. **Individual Embedding Inserts** (`worker/tasks.py:1074-1077`)
   - **Impact**: N individual `db.add()` calls instead of bulk
   - **Effect**: 10-100x slower for large documents (100+ embeddings)
   - **Solution**: Use `bulk_insert_mappings()`
   - **Expected Gain**: 10-100x faster

3. **Zero MinerU Timing Instrumentation** (`services/mineru_standalone.py:173-340`)
   - **Impact**: Cannot identify which stage is slow (layout, OCR, etc.)
   - **Effect**: Blind to actual bottlenecks inside GPU processing
   - **Solution**: Add stage-level timing
   - **Expected Gain**: Infinite visibility, zero perf cost

#### ‚ö†Ô∏è **High-Impact Issues**

4. **Request ID Mismatches** (`services/mineru_client.py:144-151`)
   - **Cause**: Shared result queue, order-dependent retrieval
   - **Effect**: Head-of-line blocking, cascading delays
   - **Solution**: Request-specific Redis keys
   - **Expected Gain**: Eliminate mismatches

5. **GPU Memory Race Conditions** (`services/mineru_standalone.py:192-198`)
   - **Cause**: Check-then-use pattern without reservation
   - **Effect**: OOM errors after check passes, retry storms
   - **Solution**: Redis-based GPU memory reservation
   - **Expected Gain**: Eliminate OOM errors

6. **No Backpressure** (`worker/tasks.py:668-679`)
   - **Cause**: Always queue embeddings regardless of system load
   - **Effect**: Ollama overload during PDF processing bursts
   - **Solution**: Check queue depth before queueing
   - **Expected Gain**: Prevent service degradation

### Performance Metrics & Observability

#### Current Metrics (Existing)
- ‚úÖ Task-level progress tracking (`worker/tasks.py:62-126`)
- ‚úÖ GPU memory availability checks (`utils/gpu_utils.py`)
- ‚úÖ Batch writer performance (`services/batch_writer.py`)
- ‚úÖ Connection pool monitoring (`db/session.py:284-387`)
- ‚úÖ Document processing counters (`core/monitoring.py`)

#### Missing Metrics (To Add)
- ‚ùå MinerU stage-level timing (layout, OCR, markdown generation)
- ‚ùå Pages-per-minute throughput
- ‚ùå GPU utilization during processing
- ‚ùå End-to-end pipeline latency (file ‚Üí embeddings)
- ‚ùå Queue wait time analysis
- ‚ùå Embeddings-per-second throughput
- ‚ùå Database query performance tracking

### Resource Utilization

| Resource | Capacity | Typical Usage | Saturation Point |
|----------|----------|---------------|------------------|
| GPU Memory | 24 GB | 1-8 GB per PDF | > 20 GB (OOM risk) |
| CPU | 16 cores | 30-60% | > 90% (throttling) |
| Redis Memory | 2 GB | 100-500 MB | > 1.5 GB (eviction) |
| DB Connections | 20 + 10 overflow | 5-15 | > 25 (timeout) |
| Ollama Concurrency | 8 concurrent | 4-8 | > 8 (queueing) |
| MinerU Instances | 3 GPU instances | 1-3 active | > 3 (queuing) |

### Queue Depths (Healthy vs Warning)

| Queue | Healthy | Warning | Critical | Recovery |
|-------|---------|---------|----------|----------|
| `mineru_requests_N` | < 5 | 5-20 | > 20 | Clear or add instance |
| `celery` (PDF tasks) | < 50 | 50-200 | > 200 | Scale workers |
| `embeddings` | < 100 | 100-500 | > 500 | Backpressure needed |
| `batch_writer` (memory) | < 5000 | 5000-9000 | > 9000 | Data loss risk |

### Optimization Roadmap

> **Phase 1: Observability** (Week 1)
> - Add MinerU stage timing
> - Add embeddings/sec metrics
> - Add end-to-end latency tracking
> - Add queue wait time metrics

> **Phase 2: Quick Wins** (Week 1)
> - Bulk embedding inserts (10-100x gain)
> - Enable async file I/O (10-20% gain)
> - Enable batch DB writes (10-20% gain)
> - Cache GPU memory checks (eliminate overhead)

> **Phase 3: Architectural** (Week 2)
> - Request-specific Redis keys
> - Async Redis communication (2-3x gain)
> - GPU memory reservation system
> - Backpressure for embedding queue

> **Phase 4: Validation** (Week 3)
> - Performance regression tests
> - Load testing (100+ documents)
> - Metrics validation
> - Document baselines

**Expected Combined Impact: 20-50x improvement**

---

> **‚ö†Ô∏è MAINTENANCE NOTE**: Update this section whenever:
> - Adding new pipeline stages
> - Changing queue configurations
> - Modifying processing logic
> - Implementing performance optimizations
> - Discovering new bottlenecks

## Project Structure

```
/mnt/datadrive_m2/codex_librarian/
‚îú‚îÄ‚îÄ src/pdf_to_markdown_mcp/
‚îÇ   ‚îú‚îÄ‚îÄ api/           # FastAPI endpoints
‚îÇ   ‚îú‚îÄ‚îÄ worker/        # Celery tasks
‚îÇ   ‚îú‚îÄ‚îÄ services/      # Business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mineru_standalone.py  # GPU service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config_service.py     # Config persistence
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ document_service.py   # Document CRUD
‚îÇ   ‚îú‚îÄ‚îÄ db/            # Database models, migrations
‚îÇ   ‚îî‚îÄ‚îÄ config.py      # Settings, environment
‚îú‚îÄ‚îÄ systemd/           # Service definitions
‚îú‚îÄ‚îÄ scripts/           # Monitoring & diagnostics
‚îú‚îÄ‚îÄ alembic/           # Database migrations
‚îú‚îÄ‚îÄ .env               # Environment configuration
‚îî‚îÄ‚îÄ CLAUDE.md          # This file
```

**Key Paths:**
- Input PDFs: `/mnt/codex_fs/research/codex_articles/`
- Output Markdown: `/mnt/codex_fs/research/librarian_output/`
- MinerU Log: `/tmp/mineru.log`
- Celery Log: `/var/log/celery-worker.log`

## Essential Commands

### Service Management
```bash
# Status check
./scripts/check_services.sh

# View logs
./scripts/view_logs.sh all          # All services
./scripts/view_logs.sh api          # API only
./scripts/view_logs.sh worker       # Celery worker
./scripts/view_logs.sh follow       # Real-time

# Restart services
./scripts/restart_services.sh

# Stop services
./scripts/stop_services.sh
```

### Manual Service Control
```bash
# Start services
sudo systemctl start pdf-celery-worker
sudo systemctl start pdf-celery-beat
sudo systemctl start pdf-api-server

# Check status
systemctl status pdf-api-server
systemctl status pdf-celery-worker

# View systemd logs
sudo journalctl -u pdf-api-server -f
```

### Database Access
```bash
# Connect to database
export PGPASSWORD='YOUR_DB_PASSWORD'
psql -h 192.168.1.104 -U codex_librarian -d codex_librarian

# Check document status
psql -h 192.168.1.104 -U codex_librarian -d codex_librarian -c "
SELECT conversion_status, COUNT(*)
FROM documents
GROUP BY conversion_status;"
```

### GPU Monitoring
```bash
# Real-time GPU status
watch -n 1 nvidia-smi

# Check MinerU GPU usage
nvidia-smi | grep python

# Monitor MinerU processing
tail -f /tmp/mineru.log | grep -E "Layout|MFD|OCR|Completed"
```

### Redis Queue Inspection
```bash
# Check queue lengths
redis-cli llen mineru_requests
redis-cli llen mineru_results
redis-cli llen celery
redis-cli llen embeddings

# Clear queues (emergency only)
redis-cli FLUSHDB
```

### Health Checks
```bash
# Quick health check
./scripts/quick_health_check.sh

# Full diagnostic
python scripts/system_diagnostic.py

# API health
curl http://localhost:8000/health | jq .
curl http://192.168.1.110:8000/health | jq .
```

## Configuration API

### Get Current Configuration
```bash
curl http://192.168.1.110:8000/api/v1/configuration | jq .
```

### Update Watch Directories
```bash
# Single directory
curl -X POST http://192.168.1.110:8000/api/v1/configure \
  -H "Content-Type: application/json" \
  -d '{
    "watch_directories": ["/mnt/codex_fs/research/"],
    "restart_watcher": true
  }'

# Multiple directories
curl -X POST http://192.168.1.110:8000/api/v1/configure \
  -H "Content-Type: application/json" \
  -d '{
    "watch_directories": [
      "/mnt/codex_fs/research/",
      "/mnt/codex_fs/papers/",
      "/mnt/codex_fs/books/"
    ],
    "restart_watcher": true
  }'
```

### Reset to Defaults
```bash
curl -X POST http://192.168.1.110:8000/api/v1/configuration/reset
```

## Code Conventions

### Python Style
- Use type hints for all function signatures
- Async functions for I/O operations
- Pydantic models for validation
- FastAPI dependency injection for services

### Error Handling
- Log errors with context (document ID, filename)
- Update database status on failures
- Retry logic for transient failures
- Never swallow exceptions silently

### Database Transactions
- Use context managers: `with get_db_session() as db:`
- Commit explicitly after successful operations
- Rollback on errors
- Always close sessions

### Task Queue Best Practices
- Idempotent tasks (safe to retry)
- Store task_id in database for tracking
- Set reasonable timeouts
- Log task start/completion

## Deployment Workflow

### Initial Deployment

**1. Install systemd services:**
```bash
sudo cp systemd/*.service /etc/systemd/system/
sudo systemctl daemon-reload
sudo systemctl enable pdf-api-server pdf-celery-worker pdf-celery-beat
```

**2. Create log files:**
```bash
sudo touch /var/log/pdf-api-server.log
sudo touch /var/log/celery-worker.log
sudo touch /var/log/celery-beat.log
sudo chown ladvien:ladvien /var/log/pdf-*.log /var/log/celery-*.log
sudo chmod 644 /var/log/pdf-*.log /var/log/celery-*.log
```

**3. Configure firewall:**
```bash
# UFW
sudo ufw allow from 192.168.1.0/24 to any port 8000
sudo ufw reload

# Or firewalld
sudo firewall-cmd --permanent --add-port=8000/tcp
sudo firewall-cmd --reload
```

**4. Start services:**
```bash
sudo systemctl start pdf-celery-worker
sleep 3
sudo systemctl start pdf-celery-beat
sleep 2
sudo systemctl start pdf-api-server
sleep 3
./scripts/check_services.sh
```

**5. Verify deployment:**
```bash
curl http://localhost:8000/health | jq .
curl http://192.168.1.110:8000/health | jq .
```

### Configuration Updates

**Priority order (highest to lowest):**
1. Runtime API changes ‚Üí `server_configuration` table
2. Database config ‚Üí Persists across restarts
3. `.env` file ‚Üí Initial/default configuration

**Update config via API:**
```bash
curl -X POST http://192.168.1.110:8000/api/v1/configure \
  -H "Content-Type: application/json" \
  -d '{"watch_directories": ["/new/path/"], "restart_watcher": true}'
```

## Verification Checklist

### Critical Success Criteria
- ‚úÖ GPU actively processing PDFs (>1GB GPU memory during processing)
- ‚úÖ Markdown files created in OUTPUT_DIR
- ‚úÖ Vector embeddings stored in PostgreSQL
- ‚úÖ No errors in service logs

### Step 1: Verify GPU Usage
```bash
nvidia-smi                           # Check GPU available
nvidia-smi | grep python             # Check MinerU using GPU
ps aux | grep mineru_standalone      # Verify process running
```

**Expected:** Python process using 1-8GB GPU memory during processing

### Step 2: Verify Services
```bash
systemctl status pdf-api-server      # Should be active (running)
systemctl status pdf-celery-worker   # Should be active (running)
systemctl status pdf-celery-beat     # Should be active (running)
redis-cli ping                       # Should return PONG
curl http://localhost:11434/api/tags | jq '.models[].name' | grep nomic
```

### Step 3: Verify Database
```bash
export PGPASSWORD='YOUR_DB_PASSWORD'

# Check document statistics
psql -h 192.168.1.104 -U codex_librarian -d codex_librarian -c "
SELECT conversion_status, COUNT(*), MAX(updated_at) as last_update
FROM documents
GROUP BY conversion_status;"

# Check embeddings
psql -h 192.168.1.104 -U codex_librarian -d codex_librarian -c "
SELECT COUNT(*) as total_embeddings,
       COUNT(DISTINCT document_id) as docs_with_embeddings,
       MAX(created_at) as most_recent
FROM document_embeddings;"
```

### Step 4: Verify File System
```bash
# Check output directory
ls -la /mnt/codex_fs/research/librarian_output/

# Check recent files (last hour)
find /mnt/codex_fs/research/librarian_output -name "*.md" -mmin -60 -ls

# Verify content
head -100 $(ls -t /mnt/codex_fs/research/librarian_output/*.md | head -1)
```

### Step 5: Monitor Processing
```bash
# Check MinerU activity
tail -100 /tmp/mineru.log | grep "Completed job"

# Real-time monitoring
tail -f /tmp/mineru.log | grep -E "Layout|MFD|OCR|Processing"

# Check queue status
redis-cli llen mineru_requests
redis-cli llen mineru_results
```

## Troubleshooting

### GPU Not Being Used

**Symptoms:**
- Processing slow
- No GPU memory in `nvidia-smi`
- CPU processing in logs

**Fix:**
```bash
# Kill and restart with GPU
pkill -f mineru_standalone.py
source .venv/bin/activate
CUDA_VISIBLE_DEVICES=0 REDIS_PORT=6379 \
  nohup python src/pdf_to_markdown_mcp/services/mineru_standalone.py \
  > /tmp/mineru.log 2>&1 &

# Verify CUDA
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

### Markdown Files Not Created

**Symptoms:**
- Documents marked "completed" but no files
- Empty OUTPUT_DIR

**Fix:**
```bash
# Check permissions
touch /mnt/codex_fs/research/librarian_output/test.txt && \
  rm /mnt/codex_fs/research/librarian_output/test.txt

# Check environment
grep OUTPUT_DIRECTORY .env

# Restart worker
sudo systemctl restart pdf-celery-worker
```

### Embeddings Not Generated

**Symptoms:**
- Empty document_embeddings table
- Documents stuck in "pending" status

**Fix:**
```bash
# Restart Ollama
sudo systemctl restart ollama
ollama pull nomic-embed-text

# Test embedding generation via system diagnostic
python scripts/system_diagnostic.py

# Or check embeddings health via API
curl http://localhost:8000/health | jq '.components.embeddings'
```

### Request ID Mismatch

**Symptoms:**
- "Request ID mismatch" errors
- Multiple retries failing

**Fix:**
```bash
# Clear queues and restart
redis-cli FLUSHDB
pkill -f mineru_standalone.py
sudo systemctl restart pdf-celery-worker

# Start fresh MinerU
source .venv/bin/activate
REDIS_PORT=6379 nohup python src/pdf_to_markdown_mcp/services/mineru_standalone.py \
  > /tmp/mineru.log 2>&1 &
```

### Full System Restart

**Use this for major issues:**
```bash
# 1. Stop all services
sudo systemctl stop pdf-celery-worker pdf-celery-beat
pkill -f mineru_standalone.py

# 2. Clear caches
find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null
> /tmp/mineru.log

# 3. Clear Redis
redis-cli FLUSHDB

# 4. Start services in order
source .venv/bin/activate

# Start MinerU
CUDA_VISIBLE_DEVICES=0 REDIS_PORT=6379 \
  nohup python src/pdf_to_markdown_mcp/services/mineru_standalone.py \
  > /tmp/mineru.log 2>&1 &
sleep 5

# Start Celery
sudo systemctl start pdf-celery-worker pdf-celery-beat

# Verify
./scripts/check_services.sh
```

### Reset Stuck Documents

**For documents stuck in "processing" state:**
```bash
export PGPASSWORD='YOUR_DB_PASSWORD'
psql -h 192.168.1.104 -U codex_librarian -d codex_librarian << EOF
-- Reset documents stuck in processing
UPDATE documents
SET conversion_status = 'pending'
WHERE conversion_status = 'processing'
AND updated_at < NOW() - INTERVAL '1 hour';

-- Reset stuck embeddings
UPDATE document_content
SET embedding_status = 'pending'
WHERE embedding_status = 'processing'
AND embedding_generated_at < NOW() - INTERVAL '1 hour';
EOF
```

## Health Metrics

| Metric | Healthy | Warning | Critical |
|--------|---------|---------|----------|
| GPU Memory | 1-8 GB | < 500 MB | 0 MB |
| Pending Docs | < 100 | 100-500 | > 500 |
| Processing Speed | > 10 pages/min | 5-10 | < 5 |
| Embeddings/Doc | 20-200 | < 10 | 0 |
| Queue Lengths | < 50 | 50-200 | > 200 |
| Recent Files (1hr) | > 5 | 1-5 | 0 |
| Error Rate | < 5% | 5-20% | > 20% |

## Custom Shortcuts

### QCHECK - Quick Health Check
```bash
# Run comprehensive health check
./scripts/quick_health_check.sh
```

### QGPU - GPU Status
```bash
# Check GPU usage and MinerU process
nvidia-smi && nvidia-smi | grep python
```

### QLOGS - View Recent Errors
```bash
# Show recent errors from all services
tail -100 /var/log/celery-worker.log | grep -i error
tail -100 /tmp/mineru.log | grep -i error
```

### QSTATS - Database Statistics
```bash
# Show document and embedding statistics
export PGPASSWORD='YOUR_DB_PASSWORD'
psql -h 192.168.1.104 -U codex_librarian -d codex_librarian -c "
SELECT 
    conversion_status,
    COUNT(*) as count,
    MAX(updated_at) as last_update
FROM documents
GROUP BY conversion_status;"
```

### QRESET - Emergency Reset
```bash
# Full system restart (use carefully)
./scripts/restart_services.sh
```

## Testing

### Test Suite Overview

The project has **three distinct test categories**:

1. **Unit Tests** (fast, mocked) - SQLite, CPU-only, no external services
2. **Integration Tests** (real services) - PostgreSQL, GPU, Redis, Ollama
3. **End-to-End Tests** (full pipeline) - Complete workflow validation

### Quick Test Commands

```bash
# Fast unit tests (10-30 seconds) - no prerequisites
./scripts/test_unit.sh

# Integration tests (2-5 minutes) - requires GPU + services
./scripts/test_integration.sh

# Validate environment before integration tests
./scripts/validate_test_env.sh

# Run specific test categories
pytest -m unit              # Unit tests only
pytest -m integration       # Integration tests only
pytest -m e2e               # End-to-end tests only
pytest -m gpu               # GPU-dependent tests only

# Run with coverage
./scripts/test_unit.sh --cov
```

### Integration Test Prerequisites

Integration and e2e tests require:
- ‚úÖ NVIDIA GPU with CUDA 12.4+
- ‚úÖ PostgreSQL 17+ with PGVector (192.168.1.104)
- ‚úÖ Redis server (localhost:6379)
- ‚úÖ Ollama with nomic-embed-text model
- ‚úÖ MinerU standalone service running

**Validate prerequisites:**
```bash
./scripts/validate_test_env.sh
```

Example output:
```
‚úÖ PostgreSQL connection
‚úÖ PGVector extension
‚úÖ nvidia-smi available
  GPU: NVIDIA GeForce RTX 3090
  Memory: 24576 MiB
‚úÖ CUDA available in Python
  CUDA Version: 12.4
‚úÖ Redis server
‚úÖ Ollama service
‚úÖ Ollama model (nomic-embed-text)
‚ö†Ô∏è  MinerU process running (optional)
‚ö†Ô∏è  Celery worker (optional)

All required prerequisites met!
```

### Key Differences: Unit vs Integration Tests

| Aspect | Unit Tests | Integration Tests |
|--------|-----------|-------------------|
| Database | SQLite (mocked) | PostgreSQL + PGVector |
| GPU | CPU-only | Real CUDA GPU |
| Services | All mocked | Real Redis, Ollama, MinerU |
| Duration | 10-30 seconds | 2-5 minutes |
| Purpose | Fast feedback | Real validation |
| CI/CD | Every commit | Pre-deployment |

### Test Organization

```
tests/
‚îú‚îÄ‚îÄ unit/              # Fast mocked tests
‚îú‚îÄ‚îÄ integration/       # Real service tests
‚îú‚îÄ‚îÄ e2e/              # Full pipeline tests
‚îú‚îÄ‚îÄ fixtures/
‚îÇ   ‚îú‚îÄ‚îÄ real_database.py    # Real PostgreSQL
‚îÇ   ‚îú‚îÄ‚îÄ real_gpu.py          # GPU validation
‚îÇ   ‚îî‚îÄ‚îÄ real_services.py     # Real Redis/Ollama
‚îî‚îÄ‚îÄ README.md         # Comprehensive test documentation
```

**For complete testing documentation**, see **`tests/README.md`**

### Common Test Issues

**GPU tests failing:**
```bash
# Check GPU availability
nvidia-smi
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"

# Verify environment
export CUDA_VISIBLE_DEVICES=0
export MINERU_DEVICE_MODE=cuda
```

**Integration tests skipped:**
```bash
# Check what's missing
./scripts/validate_test_env.sh

# Start missing services
sudo systemctl start redis
sudo systemctl start ollama
ollama pull nomic-embed-text
```

**See `tests/README.md` for detailed troubleshooting.**

## Do Not Section

### Never
- ‚ùå Edit files in `/mnt/codex_fs/` directly (read-only source data)
- ‚ùå Modify systemd service files without testing
- ‚ùå Clear Redis queues during active processing
- ‚ùå Change database schema without Alembic migration
- ‚ùå Commit `.env` file to version control
- ‚ùå Run MinerU multiple times (only one instance)

### Always
- ‚úÖ Use Alembic for database migrations
- ‚úÖ Test configuration changes in development first
- ‚úÖ Check GPU availability before processing
- ‚úÖ Monitor logs after deployment
- ‚úÖ Verify backups before major changes
- ‚úÖ Document configuration changes
- ‚úÖ Update this CLAUDE.md when adding features

## Environment Details

- **Host**: 192.168.1.110
- **API Port**: 8000
- **Database**: 192.168.1.104:5432
- **Redis**: localhost:6379 (Docker)
- **Ollama**: localhost:11434
- **User**: ladvien
- **Python**: 3.11+ (.venv)
- **GPU**: NVIDIA RTX 3090 (CUDA 12.4+)

## Key Features

- ‚úÖ MCP-style configuration via API
- ‚úÖ Database-persisted configuration
- ‚úÖ Single source of truth (WATCH_DIRECTORIES + OUTPUT_DIRECTORY)
- ‚úÖ Network accessible API
- ‚úÖ GPU-accelerated processing (5-10x faster)
- ‚úÖ Production-ready systemd services
- ‚úÖ Multiple watch directories support
- ‚úÖ Auto-restart on failures
- ‚úÖ Comprehensive logging

## Getting Help

**If experiencing issues:**

1. Run diagnostics:
   ```bash
   python scripts/system_diagnostic.py > diagnostic_report.txt
   ```

2. Collect logs:
   ```bash
   ./scripts/view_logs.sh all > all_logs.txt
   tail -100 /tmp/mineru.log > mineru_log.txt
   nvidia-smi > gpu_state.txt
   ```

3. Check database:
   ```bash
   psql -h 192.168.1.104 -U codex_librarian -d codex_librarian -c "\conninfo"
   ```

4. Review this document for troubleshooting steps

## MCP Semantic Search Server

### Overview

The MCP server provides a `search_library` tool that enables LLMs (like Claude) to semantically search the document library using hybrid search (vector similarity + BM25 keyword matching).

**Key Features:**
- üîç Hybrid search (vector + keyword) via Reciprocal Rank Fusion
- üìÑ Returns markdown file paths for direct document access
- ‚ö° Sub-second response times
- üîß Zero .env dependency - all configuration via MCP client
- üìÖ Date range filtering support
- üéØ Configurable similarity thresholds

### Quick Setup

**1. Add to Claude Desktop config:**

```json
{
  "mcpServers": {
    "codex-librarian": {
      "command": "uv",
      "args": ["run", "python", "-m", "pdf_to_markdown_mcp.mcp.server"],
      "cwd": "/mnt/datadrive_m2/codex_librarian",
      "env": {
        "DATABASE_URL": "postgresql://codex_librarian:PASSWORD@192.168.1.104:5432/codex_librarian",
        "OLLAMA_URL": "http://localhost:11434"
      }
    }
  }
}
```

**2. Restart Claude Desktop**

**3. Test the tool:**
```
Search my library for "neural networks"
```

### Configuration Reference

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `DATABASE_URL` | ‚úÖ | - | PostgreSQL connection string |
| `OLLAMA_URL` | No | `http://localhost:11434` | Ollama API endpoint |
| `OLLAMA_MODEL` | No | `nomic-embed-text` | Embedding model |
| `DB_POOL_MIN_SIZE` | No | `2` | Min connections |
| `DB_POOL_MAX_SIZE` | No | `10` | Max connections |
| `MCP_LOG_LEVEL` | No | `INFO` | Logging level |
| `SEARCH_DEFAULT_LIMIT` | No | `10` | Default results |
| `SEARCH_MAX_LIMIT` | No | `50` | Max results |
| `SEARCH_DEFAULT_SIMILARITY` | No | `0.7` | Similarity threshold |

### Tool Usage

```python
search_library(
    query: str,              # Natural language query (required)
    limit: int = 10,         # Max results (1-50)
    min_similarity: float = 0.7,  # Similarity threshold (0.0-1.0)
    tags: list[str] = None,  # Tag filters (future)
    date_from: str = None,   # ISO 8601 date filter
    date_to: str = None      # ISO 8601 date filter
)
```

**Response includes:**
- `document_id` - Database ID
- `filename` - Original PDF filename
- `markdown_path` - Path to converted markdown file
- `similarity_score` - Relevance score (0.0-1.0)
- `excerpt` - Text snippet from document
- `page_number` - Source page number
- `created_at` - Processing timestamp

### Troubleshooting

**MCP server won't start:**
```bash
# Check configuration
cat ~/.config/claude/claude_desktop_config.json

# Test database connection
psql "postgresql://user:pass@host:5432/db" -c "SELECT 1"

# Verify Ollama is running
curl http://localhost:11434/api/tags
```

**No results found:**
```sql
-- Check document count
psql -h 192.168.1.104 -U codex_librarian -d codex_librarian -c "
SELECT COUNT(*) FROM documents WHERE conversion_status = 'completed';
SELECT COUNT(*) FROM document_embeddings;
"
```

**Slow searches:**
```sql
-- Ensure HNSW index exists
SELECT indexname FROM pg_indexes WHERE tablename = 'document_embeddings';

-- Create if missing
CREATE INDEX idx_embeddings_hnsw ON document_embeddings
  USING hnsw (embedding vector_cosine_ops)
  WITH (m = 16, ef_construction = 64);
```

### Documentation

See **`docs/MCP_SETUP.md`** for:
- Detailed setup instructions
- Configuration examples
- Performance tuning
- Security considerations
- Advanced usage

### Architecture

The MCP server runs as a separate process from the FastAPI server:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Claude Desktop       ‚îÇ
‚îÇ (MCP Client)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ stdio
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FastMCP Server       ‚îÇ
‚îÇ - search_library     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îú‚îÄ‚îÄ‚ñ∂ PostgreSQL + PGVector (vector similarity)
           ‚îî‚îÄ‚îÄ‚ñ∂ Ollama (query embeddings)
```

## Notes for AI Assistants

When helping with this project:
- Always check GPU status before processing tasks
- Verify services are running before suggesting code changes
- Use provided scripts for diagnostics and monitoring
- Follow the deployment workflow for new features
- Test database changes with Alembic migrations first
- Monitor logs after making changes
- Use the verification checklist after deployments
- **For MCP issues**: Check `docs/MCP_SETUP.md` first